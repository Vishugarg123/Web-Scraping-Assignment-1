{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa14b64f",
   "metadata": {},
   "source": [
    "# Write python program to display all the hearder tags from wikipedia.org."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb3221c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95946177",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a9f47b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "849835ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "page=requests.get('https://www.wikipedia.org/')\n",
    "page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc44f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "bs=BeautifulSoup(page.content)\n",
    "bs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc0ecb45",
   "metadata": {},
   "outputs": [],
   "source": [
    "titles=bs.find_all(['h1','h2','h3','h4','h5','h6','h7'])\n",
    "print(\"List of all header tags:\", *titles,sep='\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2366dd84",
   "metadata": {},
   "source": [
    "# Que=2 Program to Display top rated 100 Movies data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7bc3df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# Download IMDB's Top 100 data\n",
    "url = 'http://www.imdb.com/chart/top'\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, 'lxml')\n",
    "\n",
    "movies = soup.select('td.titleColumn')\n",
    "ratings = [b.attrs.get('data-value') for b in soup.select('td.posterColumn span[name=ir]')]\n",
    "\n",
    "imdb = []\n",
    "\n",
    "for index in range(0, 101):\n",
    "    movie_string = movies[index].get_text()\n",
    "    movie = (' '.join(movie_string.split()).replace('.', ''))\n",
    "    movie_title = movie[len(str(index))+1:-7]\n",
    "    year = re.search('\\((.*?)\\)', movie_string).group(1)\n",
    "    place = movie[:len(str(index))-(len(movie))]\n",
    "    data = {\"movie_name\": movie_title,\n",
    "            \"rating\": ratings[index],\n",
    "            \"year\": year}\n",
    "    imdb.append(data)\n",
    "    \n",
    "df=pd.DataFrame(data=imdb)\n",
    "df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d278894e",
   "metadata": {},
   "source": [
    "# Que=4 Program to to scrape product details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c0a187",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "page=requests.get(\"https://meesho.com/bags-ladies/pl/p7vbp\")\n",
    "page\n",
    "\n",
    "soup=BeautifulSoup(page.content)\n",
    "soup\n",
    "\n",
    "name = soup.find('p',class_=\"Text__StyledText-sc-oo0kvp-0 bWSOET NewProductCard__ProductTitle_Desktop-sc-j0e7tu-4 cQhePS NewProductCard__ProductTitle_Desktop-sc-j0e7tu-4 cQhePS\")\n",
    "name.text\n",
    "\n",
    "\n",
    "product = []\n",
    "for i in soup.find_all('p',class_=\"Text__StyledText-sc-oo0kvp-0 bWSOET NewProductCard__ProductTitle_Desktop-sc-j0e7tu-4 cQhePS NewProductCard__ProductTitle_Desktop-sc-j0e7tu-4 cQhePS\"):\n",
    "    product.append(i.text)\n",
    "    \n",
    "product\n",
    "\n",
    "\n",
    "price = []\n",
    "\n",
    "for i in soup.find_all('h5',color=\"greyBase\"):\n",
    "    price.append(i.text)\n",
    "    \n",
    "price    \n",
    "\n",
    "disct = []\n",
    "\n",
    "for i in soup.find_all('div', class_=\"Card__BaseCard-sc-b3n78k-0 iJCKg NewProductCard__DiscountRow-sc-j0e7tu-15 kUcVjG NewProductCard__DiscountRow-sc-j0e7tu-15 kUcVjG\"):\n",
    "    disct.append(i.text)\n",
    "    \n",
    "disct    \n",
    "\n",
    "\n",
    "df = pd.DataFrame({'Product':product,'Price':price,'Distcount':disct})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b58afe2e",
   "metadata": {},
   "source": [
    "# Que=3 Program to Display top rated 100 Indian Movies data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b17cc434",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "page=requests.get('https://www.imdb.com/india/top-rated-indian-movies/')\n",
    "page\n",
    "\n",
    "soup=BeautifulSoup(page.content)\n",
    "soup\n",
    "\n",
    "india = []\n",
    "\n",
    "for i in soup.find_all('td',class_=\"titleColumn\"):\n",
    "    india.append(i.text.replace('\\n',''))\n",
    "    \n",
    "india\n",
    "\n",
    "\n",
    "indianmov = india[0:100]\n",
    "indianmov\n",
    "\n",
    "year = []\n",
    "\n",
    "for i in soup.find_all('span',class_=\"secondaryInfo\"):\n",
    "    year.append(i.text)\n",
    "year\n",
    "\n",
    "yr = year[0:100]\n",
    "yr\n",
    "\n",
    "rating = []\n",
    "\n",
    "for i in soup.find_all('td',class_=\"ratingColumn imdbRating\"):\n",
    "    rating.append(i.text.replace('\\n',''))\n",
    "rating\n",
    "rt= rating[0:100]\n",
    "rt\n",
    "\n",
    "df = pd.DataFrame({'Indian_mov':indianmov,'Year':yr,'Rating':rt})\n",
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37908716",
   "metadata": {},
   "source": [
    "# Que=5 Program of cicket men's team details"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c64fd403",
   "metadata": {},
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "page=requests.get('https://www.icc-cricket.com/rankings/mens/team-rankings/odi')\n",
    "page\n",
    "\n",
    "soup=BeautifulSoup(page.content)\n",
    "soup\n",
    "\n",
    "team=[]\n",
    "\n",
    "for i in soup.find_all('span',class_=\"u-hide-phablet\"):\n",
    "    team.append(i.text)\n",
    "team\n",
    "\n",
    "tm=team[0:10]\n",
    "\n",
    "print(tm)\n",
    "\n",
    "\n",
    "match = []\n",
    "\n",
    "for i in soup.find_all('td',class_=\"rankings-block__banner--matches\"):\n",
    "   match.append(i.text)\n",
    "print(match) \n",
    "\n",
    "\n",
    "points = []\n",
    "\n",
    "for i in soup.find_all('td',class_=\"rankings-block__banner--points\"):\n",
    "    points.append(i.text)\n",
    "print(points)\n",
    "\n",
    "new_list = []\n",
    "\n",
    "for i in soup.find_all('td',class_=\"table-body__cell u-center-text\"):\n",
    "    new_list.append(i.text)\n",
    "print(new_list)\n",
    "\n",
    "matches = []\n",
    "points = []\n",
    "\n",
    "for i in range(0,len(new_list)-1,2):\n",
    "    matches.append(new_list[i])\n",
    "    points.append(new_list[i+1])\n",
    "print(matches,points)\n",
    "\n",
    "\n",
    "rt = []\n",
    "\n",
    "for i in soup.find_all('td',class_=\"table-body__cell u-text-right rating\"):\n",
    "    rt.append(i.text)\n",
    "print(rt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f719c03",
   "metadata": {},
   "source": [
    "# Que=7 Program of all the post of coreyms sebsit7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d914986b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "page = requests.get('https://coreyms.com/')\n",
    "page\n",
    "\n",
    "\n",
    "soup = BeautifulSoup(page.content)\n",
    "soup\n",
    "\n",
    "heading = soup.find('a',class_=\"entry-title-link\")\n",
    "heading.text\n",
    "\n",
    "head = []\n",
    "\n",
    "for i in soup.find_all('a',class_=\"entry-title-link\"):\n",
    "    head.append(i.text)\n",
    "head\n",
    "\n",
    "time = soup.find('time',class_=\"entry-time\")\n",
    "time.text\n",
    "\n",
    "date = []\n",
    "\n",
    "for i in soup.find_all('time',class_=\"entry-time\"):\n",
    "    date.append(i.text)\n",
    "date\n",
    "\n",
    "content = soup.find('div',class_=\"entry-content\")\n",
    "content.text\n",
    "\n",
    "content = []\n",
    "\n",
    "for i in soup.find_all('div',class_=\"entry-content\"):\n",
    "    content.append(i.text.replace('\\n',''))\n",
    "content\n",
    "\n",
    "code = soup.find('div',class_=\"ytp-impression-link-text\")\n",
    "code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a8c335",
   "metadata": {},
   "source": [
    "# Que=8 Program to scrape house details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd118f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "page = requests.get('https://www.nobroker.in/property/sale/mumbai/multiple?searchParam=W3sibGF0IjoyMC4zNDY4NjgyLCJsb24iOjczLjk5MTgxODEsInBsYWNlSWQiOiJDaElKSy1kUFFOZDMzanNSN3FIVkFoRWRpQ1EiLCJwbGFjZU5hbWUiOiJJbmRpcmEgTmFnYXIifSx7ImxhdCI6MTkuMjI0NTI5NCwibG9uIjo3Mi44NTkzMDIsInBsYWNlSWQiOiJDaElKRlNxanZ0T3c1enNST3NoalJ6UmJQOUEiLCJwbGFjZU5hbWUiOiJKYXlhIE5hZ2FyIn0seyJsYXQiOjEyLjk3NzMwMDksImxvbiI6NzcuNTU1NTAzNywicGxhY2VJZCI6IkNoSUpueDdfcl9BOXJqc1JzdUtDdkljN1Q5USIsInBsYWNlTmFtZSI6IlJhamFqaW5hZ2FyIn1d&radius=2.0&city=mumbai&locality=Indira%20Nagar,&locality=Jaya%20Nagar,&locality=Rajajinagar')\n",
    "page\n",
    "\n",
    "soup = BeautifulSoup(page.content)\n",
    "soup\n",
    "\n",
    "house = soup.find('h2',class_=\"heading-6 font-semi-bold nb__25Cl7\")\n",
    "house.text\n",
    "\n",
    "loc = soup.find('div',class_=\"nb__nXU01\")\n",
    "loc.text\n",
    "\n",
    "\n",
    "area = soup.find('div',class_=\"nb__FfHqA\")\n",
    "area.text\n",
    "\n",
    "area = []\n",
    "for i in soup.find_all('div',class_=\"nb__FfHqA\"):\n",
    "    area.append(i.text)\n",
    "area\n",
    "\n",
    "emi = []\n",
    "price = []\n",
    "\n",
    "for i in range(0,len(cost)-1,2) :\n",
    "    emi.append(cost[i])\n",
    "    price.append(cost[i+1])\n",
    "emi,price\n",
    "\n",
    "cost = []\n",
    "for i in soup.find_all('div',class_=\"font-semi-bold heading-6\"):\n",
    "    cost.append(i.text)\n",
    "cost\n",
    "\n",
    "house = []\n",
    "\n",
    "for i in soup.find_all('h2',class_=\"heading-6 font-semi-bold nb__25Cl7\"):\n",
    "    house.append(i.text)\n",
    "house\n",
    "\n",
    "area = []\n",
    "\n",
    "for i in soup.find_all('div',class_=\"nb__FfHqA\"):\n",
    "    area.append(i.text)\n",
    "area"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4d6b8f9",
   "metadata": {},
   "source": [
    "# Que=6 Program of cicket women's team details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b19ec3a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "page = requests.get('https://www.icc-cricket.com/rankings/womens/team-rankings/odi')\n",
    "page\n",
    "\n",
    "soup = BeautifulSoup(page.content)\n",
    "soup\n",
    "\n",
    "team = soup.find('span',class_=\"u-hide-phablet\")\n",
    "team.text\n",
    "\n",
    "match = soup.find('td',class_=\"rankings-block__banner--matches\")\n",
    "match.text\n",
    "\n",
    "ma = [match.text]\n",
    "ma\n",
    "\n",
    "point = soup.find('td',class_=\"rankings-block__banner--points\")\n",
    "point.text\n",
    "\n",
    "pnt = [point.text]\n",
    "pnt\n",
    "\n",
    "rate = soup.find('td',class_=\"rankings-block__banner--rating u-text-right\")\n",
    "rate.text\n",
    "rates = [rate.text.replace('\\n','')]\n",
    "rates\n",
    "\n",
    "team = []\n",
    "for i in soup.find_all('span',class_=\"u-hide-phablet\"):\n",
    "    team.append(i.text)\n",
    "team\n",
    "tm = team[0:10]\n",
    "tm\n",
    "\n",
    "match = []\n",
    "for i in soup.find_all('td',class_=\"table-body__cell u-center-text\"):\n",
    "    match.append(i.text)\n",
    "match\n",
    "\n",
    "mat = []\n",
    "pt =[]\n",
    "for i in range(0,len(match)-1,2):\n",
    "    mat.append(match[i])\n",
    "    pt.append(match[i+1])\n",
    "mat,pt\n",
    "\n",
    "matches = ma + mat\n",
    "matches\n",
    "matche = matches[0:10]\n",
    "matche\n",
    "\n",
    "pont = pnt + pt\n",
    "pont\n",
    "\n",
    "points = pont[0:10]\n",
    "points\n",
    "\n",
    "rat = []\n",
    "for i in soup.find_all('td',class_=\"table-body__cell u-text-right rating\"):\n",
    "    rat.append(i.text)\n",
    "rat\n",
    "\n",
    "rt = rates + rat\n",
    "rt\n",
    "rates = rt[0:10]\n",
    "rates\n",
    "\n",
    "df = pd.DataFrame({\"Teams\":tm,\"Matches\":matche,\"Points\":points,\"Rating\":rates})\n",
    "df\n",
    "\n",
    "page = requests.get('https://www.icc-cricket.com/rankings/womens/player-rankings/odi')\n",
    "page\n",
    "\n",
    "soup = BeautifulSoup(page.content)\n",
    "soup\n",
    "\n",
    "player1 = soup.find('div',class_=\"rankings-block__banner--name\")\n",
    "player1.text\n",
    "\n",
    "player1 = [player1.text]\n",
    "player1\n",
    "\n",
    "player = []\n",
    "for i in soup.find_all('td',class_=\"table-body__cell name\"):\n",
    "    player.append(i.text.replace('\\n',''))\n",
    "player\n",
    "players = player1 + player[0:9]\n",
    "players\n",
    "\n",
    "team1 = soup.find('div',class_=\"rankings-block__banner--nationality\")\n",
    "tm = [team1.text.split()[0]]\n",
    "tm\n",
    "\n",
    "team = []\n",
    "for i in soup.find_all('span',class_=\"table-body__logo-text\"):\n",
    "    team.append(i.text)\n",
    "team\n",
    "teams = tm + team[0:9]\n",
    "teams\n",
    "\n",
    "rate1 = soup.find('div',class_=\"rankings-block__banner--rating\")\n",
    "rate1.text\n",
    "rate1 = [rate1.text]\n",
    "rate1\n",
    "\n",
    "ra = []\n",
    "for i in soup.find_all('td',class_=\"table-body__cell u-text-right rating\"):\n",
    "    ra.append(i.text)\n",
    "rate = ra[0:9]\n",
    "rate\n",
    "rates = rate1+rate\n",
    "rates\n",
    "\n",
    "df = pd.DataFrame({\"Bat_players\":players,\"Teams\":teams,\"Rates\":rates})\n",
    "df\n",
    "\n",
    "player = soup.find(\"span\",class_='u-hide-phablet')\n",
    "player.text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cdb79cd",
   "metadata": {},
   "source": [
    "# Que-9 program of srape details from dineout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "245170f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "page = requests.get('https://www.dineout.co.in/delhi-restaurants/buffet-special')\n",
    "page\n",
    "\n",
    "soup = BeautifulSoup(page.content)\n",
    "soup\n",
    "\n",
    "name = soup.find('a',href=\"/delhi/castle-barbeque-connaught-place-central-delhi-86792\")\n",
    "name.text\n",
    "\n",
    "rest_name = []\n",
    "for i in soup.find_all('a',class_=\"restnt-name ellipsis\"):\n",
    "    rest_name.append(i.text)\n",
    "rest_name\n",
    "\n",
    "loc = soup.find('div',class_=\"restnt-loc ellipsis\")\n",
    "loc.text\n",
    "\n",
    "loc = []\n",
    "for i in soup.find_all('div',class_=\"restnt-loc ellipsis\"):\n",
    "    loc.append(i.text)\n",
    "loc\n",
    "\n",
    "cuisin = soup.find('div',class_=\"detail-info\")\n",
    "cuisin.text.split('|')[1]\n",
    "\n",
    "cuisine = []\n",
    "for i in soup.find_all('div',class_=\"detail-info\"):\n",
    "    cuisine.append(i.text.split('|')[1])\n",
    "cuisine\n",
    "\n",
    "rate = soup.find('div',class_=\"restnt-rating rating-4\")\n",
    "rate.text\n",
    "\n",
    "rating = []\n",
    "for i in soup.find_all('div',class_=\"restnt-rating rating-4\"):\n",
    "    rating.append(i.text)\n",
    "rating\n",
    "\n",
    "image_url = []\n",
    "for i in soup.find_all('img',class_=\"no-img\"):\n",
    "    image_url.append(i['data-src'])\n",
    "image_url\n",
    "\n",
    "df = pd.DataFrame({\"Restraunt_name\":rest_name,\"Location\":loc,\"Cuisine\":cuisin,\"Rating\":rating,\"image_url\":image_url})\n",
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d21097a4",
   "metadata": {},
   "source": [
    "# Que-10 Program to scrape first 10 products."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ef1bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "page = requests.get('https://www.bewakoof.com/women-tshirts?ga_q=tshirts')\n",
    "page\n",
    "\n",
    "soup = BeautifulSoup(page.content)\n",
    "soup\n",
    "\n",
    "prdt_name = soup.find('div',class_=\"productCardDetail\")\n",
    "prdt_name.text.split('₹')[0]\n",
    "\n",
    "prdct_name = []\n",
    "for i in soup.find_all('div',class_=\"productCardDetail\"):\n",
    "    prdct_name.append(i.text.split('₹'))\n",
    "prdct_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb5d1b2f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
